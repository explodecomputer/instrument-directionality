---
title: Analysis of version 3 simulations
author: Gibran Hemani
date: 15/06/2017
---



```{r }

library(tidyverse)
load("../results/simulate3.rdata")

res <- inner_join(res, param, by="sim")
res$method <- paste0(res$Method, " - ", res$strategy)
metrics <- metrics[! apply(metrics, 1, function(x) any(is.na(x))), ]


simeval <- function(res)
{
	require(tidyverse)
	require(ggrepel)

	res$Method <- res$method

	# Organise data
	resnull <- bind_rows(
		filter(res, hypothesis=="xy", eff_x.y == 0),
		filter(res, hypothesis=="yx")
	)
	resnull_s_d <- group_by(resnull, Method, strategy, hypothesis) %>%
		summarise(fdr = sum(P < 0.05)/n())
	levels(resnull_s_d$hypothesis) <- c("No causal effect", "Reverse cause")

	resnull_s <- group_by(resnull, Method, strategy) %>%
		summarise(fdr = sum(P < 0.05)/n())

	resxy <- filter(res, hypothesis == "xy", eff_x.y != 0)
	resxy$eff_bin <- cut(resxy$eff_x.y, 3)

	resxy_s <- group_by(resxy, Method, strategy, eff_bin) %>%
		summarise(tdr=sum(P < 0.05)/n(), bias=mean(Estimate - eff_x.y), n=n(), bias_se=sd(Estimate - eff_x.y)/sqrt(n))



	temp <- resxy %>%
		group_by(Method, strategy) %>%
		summarise(power=sum(P < 0.01)/n())
	# temp <- inner_join(temp, meth, by="Method")
	# temp$Method <- as.factor(temp$Method)
	# temp$Method <- factor(temp$Method, levels=as.character(meth$Method))

	p_power <- ggplot(temp, aes(y=power, x=Method)) +
		geom_point(position=position_dodge(width=0.3), aes(colour=strategy), size=3) +
		theme(axis.text.x=element_text(angle=90, hjust=0.5, vjust=0.5)) +
		scale_colour_brewer(type="qual") +
		labs(colour="Instrument\nselection")



	temp <- resnull_s
	# temp <- inner_join(temp, meth, by="Method")
	# temp$Method <- as.factor(temp$Method)
	# temp$Method <- factor(temp$Method, levels=as.character(meth$Method))

	p_fdr <- ggplot(temp, aes(y=fdr, x=Method)) +
		geom_point(position=position_dodge(width=0.3), aes(colour=strategy), size=3) +
		theme(axis.text.x=element_text(angle=90, hjust=0.5, vjust=0.5)) +
		scale_colour_brewer(type="qual") +
		labs(colour="Instrument\nselection")


	temp1 <- resxy %>%
		group_by(Method, strategy) %>%
		summarise(power=sum(P < 0.01)/n())
	# temp1 <- inner_join(temp1, meth, by="Method")
	# temp1$Method <- as.factor(temp1$Method)
	# temp1$Method <- factor(temp1$Method, levels=as.character(meth$Method))

	temp2 <- resnull_s
	# temp2 <- inner_join(temp2, meth, by="Method")
	# temp2$Method <- as.factor(temp2$Method)
	# temp2$Method <- factor(temp2$Method, levels=as.character(meth$Method))

	temp <- merge(temp1, temp2, by=c("Method", "strategy"))
	temp$meth2 <- paste0(temp$Method, " - ", temp$strategy)
	p_powerfdr <- ggplot(subset(temp, strategy != "oracle"), aes(x=power, y=fdr)) +
		geom_point(aes(colour=strategy), size=3) +
		geom_text_repel(aes(label=Method)) +
		scale_colour_brewer(type="qual") +
		labs(x="Power (higher is better)", y="FDR (lower is better)", colour="Instrument\nselection")



	temp <- resxy %>%
		group_by(Method, strategy) %>%
		summarise(bias=mean(Estimate - eff_x.y), se=sd(Estimate-eff_x.y))
	# temp <- inner_join(temp, meth, by="Method")
	# temp$Method <- as.factor(temp$Method)
	# temp$Method <- factor(temp$Method, levels=as.character(meth$Method))

	p_bias <- ggplot(temp, aes(y=bias, x=Method)) +
		geom_point(position=position_dodge(width=0.3), aes(colour=strategy), size=3) +
		geom_errorbar(position=position_dodge(width=0.3), aes(ymin=bias-se, ymax=bias+se, colour=strategy), width=0) +
		theme(axis.text.x=element_text(angle=90, hjust=0.5, vjust=0.5)) +
		scale_colour_brewer(type="qual") +
		labs(colour="Instrument\nselection")

	return(list(p_power=p_power, p_fdr=p_fdr, p_powerfdr=p_powerfdr, p_bias=p_bias))

}


```

For each simulation find the best method


```{r }


## Predictor for best model


make_optim_dataset <- function(optvec, res, metrics, ncores)
{
	# For each method calculate the abs(bias) for each simulation
	# Fit RF of metrics against abs(bias)

	require(tidyverse)
	require(randomForest)
	require(parallel)

	res$optvec <- optvec

	temp <- subset(res, strategy != "oracle", select=c(optvec, Method, strategy, hypothesis, sim))
	temp$method <- paste0(temp$Method, " - ", temp$strategy)
	# temp$optvec <- abs(temp$Estimate - temp$eff_x.y)
	temp <- subset(temp, select=-c(Method, strategy))

	sp <- subset(temp, method == method[1], select=c(sim, hypothesis))
	sp$tt <- runif(1:nrow(sp))

	temp <- inner_join(temp, sp, by=c("hypothesis", "sim"))
	ind <- temp$tt > 0.33

	l <- split(temp[ind,], temp$method[ind])

	rf <- mclapply(l, function(x){
		message(x$method[1])
		x <- inner_join(x, metrics, by=c("hypothesis", "sim")) %>%
			dplyr::select(-c(hypothesis, sim, method, tt))

		return(randomForest(optvec ~ ., x, ntree=60, mtry=15, importance=TRUE, do.trace=TRUE))
	}, mc.cores=ncores)

	out <- list(rf=rf, sp=sp)
	return(out)
}


test_predictor <- function(rf, sp, res, metrics)
{
	temp <- subset(res, strategy != "oracle", select=c(Estimate, eff_x.y, Method, strategy, hypothesis, sim))
	temp$method <- paste0(temp$Method, " - ", temp$strategy)
	temp$bias <- abs(temp$Estimate - temp$eff_x.y)
	temp <- subset(temp, select=-c(Method, strategy, Estimate, eff_x.y))

	met <- inner_join(metrics, sp, by=c("hypothesis", "sim")) %>%
		filter(tt <= 0.33)
	nom <- names(rf)

	d <- data.frame(method=nom, rsq=NA)

	for(i in 1:length(nom))
	{
		message(nom[i])
		pr <- predict(rf[[nom[i]]], subset(met, select=-c(hypothesis, sim)))
		x <- tibble(pr=pr, hypothesis=met$hypothesis, sim=met$sim)
		y <- subset(temp, method == nom[i])
		xy <- inner_join(x, y, by=c("hypothesis", "sim"))
		d$rsq[d$method == nom[i]] <- cor(xy$pr, xy$bias)^2
	}
	return(d)
}

add_method <- function(rf, sp, res, metrics)
{
	# PREDICT BIAS FOR EVERY METHOD
	temp <- subset(res, strategy != "oracle", select=c(Estimate, eff_x.y, Method, strategy, hypothesis, sim))
	temp$method <- paste0(temp$Method, " - ", temp$strategy)
	temp$bias <- abs(temp$Estimate - temp$eff_x.y)
	temp <- subset(temp, select=-c(Method, strategy, Estimate, eff_x.y))

	met <- inner_join(metrics, sp, by=c("hypothesis", "sim")) %>%
		filter(tt <= 0.33)
	nom <- names(rf)

	d <- tibble(hypothesis=met$hypothesis, sim=met$sim)

	for(i in 1:length(nom))
	{
		message(nom[i])
		d[[nom[i]]] <- predict(rf[[nom[i]]], subset(met, select=-c(hypothesis, sim)))
	}

	sel <- tibble(
		hypothesis=met$hypothesis, sim=met$sim,
		selmethod = nom[apply(d[,-c(1,2)], 1, function(x) which.max(x))]
	)

	res2 <- inner_join(res, sel, by=c("hypothesis", "sim")) 

	res3 <- res2 %>%
		filter(selmethod == method)
	res3$method <- "MoE - RF"
	res3$strategy <- "Mixed"
	res3 <- bind_rows(res2, res3)
	return(res3)
}


opt1 <- rep(0, nrow(res))
opt1[res$hypothesis == "xy" & res$eff_x.y != 0 & res$P < 0.001] <- 1
opt1[res$hypothesis == "yx" & res$P > 0.05] <- 1
opt1[res$hypothesis == "xy" & res$eff_x.y == 0 & res$P > 0.05] <- 1
table(opt1)


rf2 <- make_optim_dataset(opt1, res, metrics, 14)
pr2 <- test_predictor(rf2$rf, rf2$sp, res, metrics)

res_rf2 <- add_method(rf2$rf, rf2$sp, res, metrics)
eval_rf2 <- simeval(res_rf2)
eval_rf2[[3]]

save(eval_rf2 file="../results/rf2.rdata")

# Make new optim instead of bias - ratio of p(xy) / p(yx)


temp <- filter(as.data.frame(res), eff_x.y != 0, strategy != "oracle") %>%
	dplyr::select(Method, P, hypothesis, strategy, sim) %>%
	spread(key=hypothesis, value=P) %>%
	mutate(xy=-log10(xy), yx=-log10(yx)) %>%
	as_data_frame

temp$xy[is.infinite(temp$xy)] <- 300
temp$yx[is.infinite(temp$yx)] <- 300
temp$rat <- temp$xy / temp$yx

group_by(temp, Method, strategy) %>%
	summarise(rat=mean(xy / yx)) %>%
	as.data.frame()

temp2 <- group_by(temp, sim) %>%
	summarise(meth=paste0(Method, " - ", strategy)[which.max(rat)][1])
table(temp2$meth)



```
