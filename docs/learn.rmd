---
title: Analysis of version 3 simulations
author: Gibran Hemani
date: 15/06/2017
---



```{r }

library(tidyverse)
source('../scripts/fun-analysis.r')
load("../results/sim3.rdata")

```

Look at the parameter space of simulations covered

Distribution of causal effects for x and y

```{r}
plot(param$nsnp_x, param$nsnp_y)

```

Variance explained in gx vs gy


1. What is the probability of a method being the best, over varying levels of pleiotropy (or something else)
	- e.g. make a plot which shows facet_x=plei, facet_y=something else, and x=outlier method and y=mean/median/mode, and then points being the size reflecting probability of a method being best
2. How much can we explain the pleiotropy of a model using summary set metrics?

```{r}
plot(var_gx.x ~ var_gx.y, param)
hist(param$prop_gx.y)
```

```{r}



parameters[sapply(parameters, is.null)] <- NULL
parameters <- lapply(parameters, function(x) 
	{
		x[sapply(x, is.null)] <- NULL
		return(x)
		})

plei <- lapply(parameters[1:50], function(x)
{
	lapply(x, process_parameters) %>% bind_rows
}) %>% bind_rows

p <- group_by(plei, sim) %>%
summarise(
	nonpl = sum((eff_d / (eff_d + eff_p)) * (eff_d / sum(eff_d)))
)

ggplot(p, aes(x=nonpl)) +
geom_density(aes(fill=grepl("x", sim)), alpha=0.5)

p <- group_by(plei, sim, w=substr(SNP, 1,1)) %>%
summarise(
	nonpl = sum((eff_d / (eff_d + eff_p)) * (eff_d / sum(eff_d)))
)

ggplot(p, aes(x=nonpl)) +
geom_density(aes(fill=grepl("x", sim)), alpha=0.5) +
facet_grid(w ~ .)


a$eff_p <- a$eff_p * 0.001

g->u->y
g->u->x

g->x
g->y




res <- inner_join(res, param, by="sim")
res$method <- paste0(res$Method, " - ", res$strategy)
metrics <- metrics[! apply(metrics, 1, function(x) any(is.na(x))), ]




opt1 <- rep(0, nrow(res))
opt1[res$hypothesis == "xy" & res$eff_x.y != 0 & res$P < 0.001] <- 1
opt1[res$hypothesis == "yx" & res$P > 0.05] <- 1
opt1[res$hypothesis == "xy" & res$eff_x.y == 0 & res$P > 0.05] <- 1
table(opt1)


rf2 <- make_optim_dataset(opt1, res, metrics, 14)
pr2 <- test_predictor(rf2$rf, rf2$sp, res, metrics)

res_rf2 <- add_method(rf2$rf, rf2$sp, res, metrics)
eval_rf2 <- simeval(res_rf2)
eval_rf2[[4]]

save(rf2, res_rf2, pr2, eval_rf2, file="../results/rf2.rdata")


res_lda1 <- add_method(lda1$rf, lda1$sp, res, metrics)
eval_lda1 <- simeval(res_lda1)
eval_lda1[[4]]


opt2 <- as.factor(opt1)
lda1 <- make_optim_dataset2(opt2, res, metrics, 2)
prlda1 <- test_predictor(lda1$rf, lda1$sp, res, metrics)

save(lda1, eval_lda1, file="../results/lda1.rdata")

x <- lda1$rf
save(x, file="temp.rdata")







# Make new optim instead of bias - ratio of p(xy) / p(yx)

temp <- filter(as.data.frame(res), eff_x.y != 0, strategy != "oracle") %>%
	dplyr::select(Method, P, hypothesis, strategy, sim) %>%
	spread(key=hypothesis, value=P) %>%
	mutate(xy=-log10(xy), yx=-log10(yx)) %>%
	as_data_frame

temp$xy[is.infinite(temp$xy)] <- 300
temp$yx[is.infinite(temp$yx)] <- 300
temp$rat <- temp$xy / temp$yx

group_by(temp, Method, strategy) %>%
	summarise(rat=mean(xy / yx)) %>%
	as.data.frame()

temp2 <- group_by(temp, sim) %>%
	summarise(meth=paste0(Method, " - ", strategy)[which.max(rat)][1])
table(temp2$meth)



```



```{r}
library(tidyverse)
load("../results/simulate3-methods.rdata")
load("../results/simulate3.rdata")
load("../results/rf2.rdata")
res <- inner_join(res, param, by="sim")
res$method <- paste0(res$Method, " - ", res$strategy)
metrics <- metrics[! apply(metrics, 1, function(x) any(is.na(x))), ]




```

