---
title: Analysis of version 3 simulations
author: Gibran Hemani
date: 15/06/2017
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}

suppressPackageStartupMessages({
	library(knitr)
	library(pROC)
	library(tidyverse)
	library(ggrepel)
})
opts_chunk$set(cache=FALSE, echo=TRUE, message=FALSE, warning=FALSE)
```

Approximately 200,000 simulations have been performed of x associating with y, simulating different samples for x and y phenotypes and extracting the relevant SNPs from each to make a 2-sample MR analysis. Simulations are performed with the following variables. The causal and reverse causal effects were estimated using Rucker, median and mode estimators

- Sample size of x and y
- Number of instruments for x
- Number of instruments for y
- Number of confounders
- Number of instruments per confounder
- Causal effect of x on y (sometimes 0)
- Causal effects of confounders on x and y
- Variance explained by x instruments on x
- Variance explained by y instruments on y
- Proportion of instruments exhibiting pleiotropy
- Variance explained by x instruments on y and vice versa
- Effect of directional pleiotropy

Instruments for each MR test were selected either by

- Top hits (significant hits for the exposure)
- Steiger (rsq snp-exposure > rsq snp-outcome)
- Heterogeneity outliers (Qpval > 0.05)
- Steiger and then heterogeneity

Then the following tests were performed

- Mean (simple, ivw fe, egger fe, ivw re, egger re)
- Median (simple, weighted, penalised)
- Mode (simple, weighted, penalised)

This gives 44 methods in total.

## Simulations

A broad range of simulations are performed, where instruments are a mixture of direct effects (with pleiotropic effects on y), reverse effects (with pleiotropic effects on x) and confounder effects that act through u on x and y. If we break down the proportion of a SNP's effect that is valid (i.e. for all effects on x and y, what proportion is acting on x), for only detected variants in a simulation - this is what it looks like


```{r}
load("../results/sim3/agg-plei_summary.rdata")
ggplot(plei_summary, aes(x=nonpl)) +
geom_density(aes(fill=grepl("x", id)), alpha=0.5) +
facet_grid(w ~ .)
```

Note that for analyses of y on x the level of pleiotropy is slightly higher because there is a set of SNPs that influence x, which influences y. The only way that reverse causal SNPs can arise for x is if they have pleiotropic effects.


These are the distributions of estimated heterogeneity Isq from the simulations

```{r}
load("../results/sim3/agg-heterogeneity.rdata")
load("../results/sim3/agg-plei_summary.rdata")

temp <- subset(heterogeneity, method == "IVW" & !steiger_filtered & !outlier_filtered & selection == "o")

temp$isq <- (temp$Q - temp$df -1) / temp$Q
temp <- subset(temp, isq >= 0)
hist(temp$isq)
```

Are there many simulations with low heterogeneity?

```{r}
table(temp$pval < 0.05)

```

What is the relationship between isq and simulated levels of pleiotropy?

```{r dev="png"}
plei <- group_by(plei_summary, id) %>%
	summarise(nonpl = sum(nonpl * effd / sum(effd)))

temp$id <- paste0(temp$id, temp$hypothesis)
temp <- inner_join(temp, plei, by="id")

summary(lm(nonpl ~ isq, temp))

p1 <- ggplot(temp, aes(x=nonpl, y=isq)) +
geom_point() +
geom_smooth()
p1
ggsave(p1, file="../images/plei_isq.png")

```

## Analysis

Define the performance of a method as its ability to detect true positives (sensitivity) and true negatives (specificity). Estimate AUROC for each method / strategy combination. Hypothesis is that under different circumstances different methods will perform better or worse.

First example: overall across all simulations how good are the methods at discriminating?

```{r}
load("rocs.rdata")
p1 <- ggplot(rocs, aes(x=method, y=auc, group=strategy, colour=strategy)) +
geom_point(position=position_dodge(width=0.3)) +
geom_errorbar(width=0, position=position_dodge(width=0.3), aes(ymin=ci_lo, ymax=ci_up)) +
theme(axis.text.x=element_text(angle=90)) +
scale_colour_brewer(type="qual") +
labs(colour="Selection") +
theme(axis.text.x=element_text(hjust=1, vjust=0.5))
p1
ggsave(p1, file="../images/rocs1.pdf")
```

Now calculate AUROC for different classes of simulation

```{r}
x <- rocs2 %>% 
group_by(nsnpbin, pbin) %>% 
arrange(desc(auc)) %>% 
slice(1)

rocs2$top <- FALSE
rocs2$top[paste(rocs2$method, rocs2$strategy, rocs2$pbin, rocs2$nsnpbin) %in% paste(x$method, x$strategy, x$pbin, x$nsnpbin)] <- TRUE

p1 <- ggplot(rocs2, aes(x=method, y=auc, group=strategy, colour=strategy, shape=top)) +
geom_point() +
facet_grid(pbin ~ nsnpbin) +
theme(axis.text.x=element_text(angle=90)) +
scale_colour_brewer(type="qual") +
labs(colour="Selection", shape="Best")
p1
ggsave(p1, file="../images/rocs2.pdf")
```

Can we demonstrate that across different simulation scenarios there are different methods that perform most accurately, thereby necessitating some automated way of choosing the most reliable method?

```{r}

y <- rocs2 %>% as_data_frame() %>%
group_by(nsnpbin, pbin) %>% 
arrange(desc(auc)) %>% 
mutate(rank=1:n(), method=as.factor(method))
y[y$rank == 1,] %>% {table(.$method, .$strategy)}

```

We find that `r length(y[y$rank == 1,] %>% {table(.$strategy)})` out of `r length(unique(rocs2$strategy))` instrument selection strategies appear in the list of top performing methods, and `r sum(y[y$rank == 1,] %>% {table(.$method)} != 0)` of the `r length(unique(rocs2$method))` methods also.


## Summary set metrics

Can the summary set metrics predict the pleiotropy of a model?

```{r}
load("../results/sim3/agg-info.rdata")
info <- subset(info, selection == "e")

info2 <- subset(info, !steiger_filtered & !outlier_filtered) %>% mutate(id=paste0(id, hypothesis))

info2 <- inner_join(info2, plei, by=c("id"))
out <- summary(lm(nonpl ~ ., subset(info2, select=-c(id.exposure, id.outcome, id, hypothesis, selection, steiger_filtered, outlier_filtered))))
o <- out$coef %>% as_data_frame %>% 
	mutate(var = rownames(out$coef), f=`t value`^2, rsq=f/(f+out$df[2]-2)) %>% slice(-1) %>%
	arrange(desc(f))
o$var <- as.factor(o$var)
o$var <- factor(o$var, levels=as.character(o$var))
p1 <- ggplot(o, aes(x=var, y=cumsum(rsq*out$r.squared/sum(rsq)))) +
geom_point(stat="identity") +
theme(axis.text.x=element_text(angle=90, hjust=1, vjust=0.5)) +
ylim(0, out$r.squared) +
labs(y="Partial r-squared", x="Metric")
p1
ggsave(p1, file="../images/info_plei.pdf")
```

The metrics have an explanatory power adjusted r-square of `r out$r.squared` of the pleiotropic value of the simulations. This opens the possibility that the summary stats themselves can explain a substantial proportion of the variance, and therefore be predictive of the model to use.


